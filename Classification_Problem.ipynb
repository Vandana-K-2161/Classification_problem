{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d285cb9-2875-4ad6-9009-dd96f9dedbe6",
   "metadata": {},
   "source": [
    "1. Loading and Preprocessing (2 marks)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11087dce-a84e-422f-9fc4-7a646e84d035",
   "metadata": {},
   "source": [
    "Load the breast cancer dataset from sklearn.\n",
    "Preprocess the data to handle any missing values and perform necessary feature scaling.\n",
    "Explain the preprocessing steps you performed and justify why they are necessary for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6a249d2-d6d0-4070-82a3-2359ce9a58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04ded0c6-eb7e-438f-ba66-087c3e7a76da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data,columns=data.feature_names)\n",
    "y = pd.Series(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e5fdb4e-10a7-44ca-a716-90dce8776584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean radius                0\n",
      "mean texture               0\n",
      "mean perimeter             0\n",
      "mean area                  0\n",
      "mean smoothness            0\n",
      "mean compactness           0\n",
      "mean concavity             0\n",
      "mean concave points        0\n",
      "mean symmetry              0\n",
      "mean fractal dimension     0\n",
      "radius error               0\n",
      "texture error              0\n",
      "perimeter error            0\n",
      "area error                 0\n",
      "smoothness error           0\n",
      "compactness error          0\n",
      "concavity error            0\n",
      "concave points error       0\n",
      "symmetry error             0\n",
      "fractal dimension error    0\n",
      "worst radius               0\n",
      "worst texture              0\n",
      "worst perimeter            0\n",
      "worst area                 0\n",
      "worst smoothness           0\n",
      "worst compactness          0\n",
      "worst concavity            0\n",
      "worst concave points       0\n",
      "worst symmetry             0\n",
      "worst fractal dimension    0\n",
      "dtype: int64\n",
      "Total missing value:  0\n"
     ]
    }
   ],
   "source": [
    "# Check any missing values\n",
    "print(X.isnull().sum())\n",
    "print(\"Total missing value: \",X.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "188f8818-5ea6-472c-8833-1037e1ddc964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48b28ec2-54e7-4cda-8f2e-c09d73fc560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting dataset for train and test\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_scaled, y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8676021b-42ad-4f0d-9c2a-0390bbc0e1ba",
   "metadata": {},
   "source": [
    "Explanation of Preprocessing Steps\n",
    "\n",
    "1. Loading the Dataset: The dataset is loaded directly from sklearn, which provides a clean and well-structured dataset without \n",
    "   missing values.\n",
    "2. Checking Missing Value: used isnull().sum() to get the number of missing values, It's important to check for missing values to \n",
    "   ensure the integrity of the dataset. And it return 0 missing values\n",
    "3. Feature Scaling: Standardization  is performed using StandardScaler.This step is crucial because many classification \n",
    "   algorithms are sensitive to the scale of the data.Standardization helps in converging faster and achieving better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8449534b-366b-4b51-875e-a87c37dd4a7b",
   "metadata": {},
   "source": [
    "2. Classification Algorithm Implementation (5 marks)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5b9a150-4d70-431a-9298-ddc312bbbacc",
   "metadata": {},
   "source": [
    "-Implement the following five classification algorithms:\n",
    "1. Logistic Regression\n",
    "2. Decision Tree Classifier\n",
    "3. Random Forest Classifier\n",
    "4. Support Vector Machine (SVM)\n",
    "5. k-Nearest Neighbors (k-NN)\n",
    "-For each algorithm, provide a brief description of how it works and why it might be suitable for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12cb8abb-8f18-47b4-a511-0276f986faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "feature_names = data.feature_names\n",
    "\n",
    "def perfom_feature_selection(X_train,X_test,y_train,k=10):\n",
    "    selector = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    selected_feature_mask = selector.get_support()\n",
    "    selected_features = feature_names[selected_feature_mask]\n",
    "    \n",
    "    feature_scores = pd.DataFrame({\n",
    "        'Feature' : feature_names,\n",
    "        'Score' : selector.scores_\n",
    "        })\n",
    "    feature_scores = feature_scores.sort_values('Score', ascending =False)\n",
    "    return X_train_selected,X_test_selected,selected_features, feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "879ee178-e562-42eb-911b-c655e1f2b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature selection\n",
    "k_features = 10\n",
    "X_train_selected,X_test_selected,selected_features,feature_scores = perfom_feature_selection(X_train,X_test,y_train, k =k_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f14fe1b8-7994-486a-ab5c-d34e59aef34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Selected Features:\n",
      "------------------------------\n",
      "1. mean radius: 482.23\n",
      "2. mean perimeter: 522.49\n",
      "3. mean area: 423.65\n",
      "4. mean concavity: 396.66\n",
      "5. mean concave points: 695.18\n",
      "6. worst radius: 645.35\n",
      "7. worst perimeter: 681.26\n",
      "8. worst area: 495.79\n",
      "9. worst concavity: 331.33\n",
      "10. worst concave points: 746.49\n"
     ]
    }
   ],
   "source": [
    "# printing selected features\n",
    "print(\"\\nTop 10 Selected Features:\")\n",
    "print(\"-\" * 30)\n",
    "for i, feature in enumerate(selected_features, 1):\n",
    "    score = feature_scores[feature_scores['Feature'] == feature]['Score'].values[0]\n",
    "    print(f\"{i}. {feature}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2511c88d-9d93-4695-b42c-0cd040e0b151",
   "metadata": {},
   "source": [
    "1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3002de84-495f-4770-9f01-e395550a2d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize and train the model\n",
    "logreg = LogisticRegression(max_iter = 10000)\n",
    "logreg.fit(X_train,y_train)\n",
    "logreg_pred = logreg.predict(X_test)\n",
    "\n",
    "logreg_accuracy = accuracy_score(y_test,logreg_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f19d3c-c3d9-4bb1-95a0-7d5a743640cb",
   "metadata": {},
   "source": [
    "Logistic Regression is a statistical method for predicting binary classes. The outcome is modeled by a logistic function, making it suitable for this binary classification task."
   ]
  },
  {
   "cell_type": "raw",
   "id": "254d5eb5-ab16-4ac5-b0b7-e33c46f4aa62",
   "metadata": {},
   "source": [
    "2. Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22288846-b195-4fbe-a131-5fa749024afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize and train the model\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train,y_train)\n",
    "dtree_pred = dtree.predict(X_test)\n",
    "\n",
    "dtree_accuracy = accuracy_score(y_test,dtree_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef38e76d-9288-4910-a0cd-2e6aed24f60b",
   "metadata": {},
   "source": [
    "Decision Trees utilize a tree-like model of decisions. They are interpretable and can capture non-linear relationships without requiring scaling."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c5b86af-4031-49c3-baf4-e69da1396b56",
   "metadata": {},
   "source": [
    "3. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f274baf-67aa-4633-a561-249028c85abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize and train the model\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test,rf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475f7ef1-1451-4dd8-8df8-f61da802e562",
   "metadata": {},
   "source": [
    "Random Forest is an ensemble learning method that builds multiple decision trees and merges them to get a more accurate and stable prediction."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7482b32-ee59-473e-9aa8-78e4db5ebeae",
   "metadata": {},
   "source": [
    "4. Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bff85b4a-e783-4c16-a63b-75f74cf5e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize and train the model\n",
    "svm = SVC()\n",
    "svm.fit(X_train,y_train)\n",
    "svm_pred = svm.predict(X_test)\n",
    "\n",
    "svm_accuracy = accuracy_score(y_test,svm_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008939b-42f3-45e6-8df0-7e759589b184",
   "metadata": {},
   "source": [
    "SVM constructs a hyperplane in a high-dimensional space to separate classes. It is effective for high-dimensional datasets and can model complex boundaries."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1686f514-62dc-4896-be73-119b0d9789ce",
   "metadata": {},
   "source": [
    "5. k-Nearest Neighbors (k-NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "781cb76c-83c8-43b4-b495-12c3dc384953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize and train the model  \n",
    "knn = KNeighborsClassifier()  \n",
    "knn.fit(X_train, y_train)  \n",
    "knn_pred = knn.predict(X_test)  \n",
    "\n",
    "knn_accuracy = accuracy_score(y_test, knn_pred)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8893dbd-434b-4b89-80e2-3809da6829b8",
   "metadata": {},
   "source": [
    "k-NN classifies instances based on their closest training examples. It works well with small datasets and is intuitive, but may struggle with larger datasets and more complex boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17f7849-60e8-402e-adb1-9f051f535f71",
   "metadata": {},
   "source": [
    "3. Model Comparison (2 marks)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7d790f5-0c1d-4069-9517-bfe7e58f38fb",
   "metadata": {},
   "source": [
    "* Compare the performance of the five classification algorithms.\n",
    "* Which algorithm performed the best and which one performed the worst?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4cf9e2f-671a-4bb4-839d-11d76ac30b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.9737\n",
      "Decision Tree: 0.9386\n",
      "Random Forest: 0.9649\n",
      "SVM: 0.9737\n",
      "k-NN: 0.9474\n",
      "\n",
      "\n",
      "Best Model: Logistic Regression with accuracy: 0.9737\n",
      "Worst Model: Decision Tree with accuracy: 0.9386\n"
     ]
    }
   ],
   "source": [
    "# Store accuracies in a dictionary for comparison  \n",
    "accuracies = {  \n",
    "    'Logistic Regression': logreg_accuracy,  \n",
    "    'Decision Tree': dtree_accuracy,  \n",
    "    'Random Forest': rf_accuracy,  \n",
    "    'SVM': svm_accuracy,  \n",
    "    'k-NN': knn_accuracy,  \n",
    "}  \n",
    "\n",
    "# Display the accuracy for each model  \n",
    "for model, accuracy in accuracies.items():  \n",
    "    print(f\"{model}: {accuracy:.4f}\")  \n",
    "print(\"\\n\")\n",
    "# Identify the best and worst performing algorithms  \n",
    "best_model = max(accuracies, key=accuracies.get)  \n",
    "worst_model = min(accuracies, key=accuracies.get)  \n",
    "\n",
    "print(f\"Best Model: {best_model} with accuracy: {accuracies[best_model]:.4f}\")  \n",
    "print(f\"Worst Model: {worst_model} with accuracy: {accuracies[worst_model]:.4f}\")  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d8fc911-7956-49ff-920b-5f5d9fa0f2e4",
   "metadata": {},
   "source": [
    "Best Model: Logistic Regression, SVM with an accuracy of 0.97.\n",
    "Worst Model: Decision Tree with an accuracy of 0.9386"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
